{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Yandex.Afisha Marketing Analytics\n",
    "\n",
    "## Step 0. Project Description\n",
    "\n",
    "This project is part of the Business Analytics course.  \n",
    "I act as an intern in the analytical department of **Yandex.Afisha**.  \n",
    "The goal is to help optimize **marketing expenses** using data from June 2017 to May 2018.\n",
    "\n",
    "**Available data:**\n",
    "- `visits_log_us.csv` â€” website visit logs  \n",
    "- `orders_log_us.csv` â€” orders data  \n",
    "- `costs_us.csv` â€” marketing expenses  \n",
    "\n",
    "**Main questions to answer:**\n",
    "1. How do users interact with the product? (DAU/WAU/MAU, sessions, retention, devices)\n",
    "2. When and how do they buy? (conversion delay, AOV, LTV, devices)\n",
    "3. How effective is marketing? (spend, CAC, ROI, payback, devices)\n",
    "\n",
    "**Tools:** Python, Pandas, NumPy, Matplotlib, Seaborn, Plotly\n",
    "\n",
    "**Format:** Jupyter Notebook with tidy code, comments, and markdown explanations.  \n",
    "\n",
    "At the end, I will provide **recommendations for the marketing team** on how to distribute advertising budgets more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Roadmap\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Data preparation â€” load datasets, fix types.\n",
    "2. Product metrics â€” DAU/WAU/MAU, sessions, retention, session length by device.\n",
    "3. Sales metrics â€” conversion delay, orders per user, AOV, LTV, AOV by device.\n",
    "4. Marketing metrics â€” spend, CAC by source, ROI by cohorts, spend by device.\n",
    "5. Conclusion â€” summary and marketing recommendations.\n",
    "\n",
    "**Deliverables:**\n",
    "\n",
    "1. Clean Jupyter Notebook with tidy code and comments.\n",
    "2. Key plots for metrics by source and device.\n",
    "3. Final recommendations for budget allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "import plotly.graph_objects as go\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set one style for graphs and charts\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "pio.templates[\"plotly_white\"].layout.update(\n",
    "    width=800,\n",
    "    height=420,\n",
    "    margin=dict(l=60, r=20, t=60, b=60),\n",
    "    yaxis=dict(tickformat=\",\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Utility functions (loader â€¢ cleaning â€¢ quick EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert column names to snake_case.\"\"\"\n",
    "    out = df.copy()\n",
    "    out.columns = (out.columns\n",
    "                  .str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(' ', '_'))\n",
    "    return out\n",
    "\n",
    "\n",
    "def explore_data(df: pd.DataFrame, name: str, n: int = 5) -> None:\n",
    "    \"\"\"Quick, repeatble EDA for any dataframe (shape, dtypes, nulls, duplicates, head/tail).\"\"\"\n",
    "    print(f\"\\n{' Exploring' + name + ' ':=^80}\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} cols\\n\")\n",
    "    print(\"Dtypes:\")\n",
    "    display(df.dtypes.to_frame('dtype'))\n",
    "\n",
    "    nulls = df.isna().sum()\n",
    "    if nulls.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        display(nulls[nulls > 0].sort_values(ascending=False).to_frame('n_missing'))\n",
    "    else:\n",
    "        print(\"\\nMissing values: none\")\n",
    "\n",
    "    print(f\"\\nDuplicate rows: {df.duplicated().sum():,}\")\n",
    "\n",
    "    print(f\"\\nHead({n}):\")\n",
    "    display(df.head(n))\n",
    "    print(f\"\\nTail ({n}):\")\n",
    "    display(df.tail(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders with types\n",
    "def load_visits(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read Visits with datetime parsing and device as category; then make snake_case.\"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=['Start Ts', 'End Ts'], dtype={'Device': 'category'})\n",
    "    return to_snake(df)\n",
    "\n",
    "def load_orders(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read Orders with datetime parsing; then make snake_case.\"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=['Buy Ts'])\n",
    "    return to_snake(df) \n",
    "\n",
    "def load_costs(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read Costs with datetime parsing; already snake_case in source, keep consistent\"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=['dt'])\n",
    "    return to_snake(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning / validation\n",
    "def clean_visits(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute session duration (min) and drop rows with negetive duration (data anomalies).\"\"\"\n",
    "    out = df.copy()\n",
    "    out['session_duration_min'] = (out['end_ts'] - out['start_ts']).dt.total_seconds() / 60\n",
    "    neg = (out['session_duration_min'] < 0).sum()\n",
    "    if neg > 0:\n",
    "        print(f\"removed {neg} rows with negative session durations.\")\n",
    "        out = out[out['session_duration_min'] >= 0]\n",
    "    return out\n",
    "\n",
    "def validate_ranges(visits: pd.DataFrame, orders: pd.DataFrame, costs: pd.DataFrame) -> None:\n",
    "    \"\"\"Simple checks: date windows and negative values in key numeric fields\"\"\"\n",
    "    print(\"\\n=== Date ranges (min -> max) ===\")\n",
    "    print(\"Visits :\", visits['start_ts'].min(), \"->\", visits['end_ts'].max())\n",
    "    print(\"Orders :\", orders['buy_ts'].min(),   \"->\", orders['buy_ts'].max())\n",
    "    print(\"Costs  :\", costs['dt'].min(),        \"->\", costs['dt'].max())\n",
    "\n",
    "    print(\"\\nNegatives check:\")\n",
    "    print(\"Orders with negative revenue:\", (orders['revenue'] < 0).sum())\n",
    "    print(\"Costs  with negative values :\", (costs['costs'] < 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaders\n",
    "visits = load_visits('visits_log_us.csv')\n",
    "orders = load_orders('orders_log_us.csv')\n",
    "costs  = load_costs('costs_us.csv')\n",
    "\n",
    "# Quick EDA per table\n",
    "explore_data(visits, 'Visits')\n",
    "explore_data(orders, 'Orders')\n",
    "explore_data(costs,  'Costs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Basic data cleaning & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates \n",
    "print('Duplicate rows:')\n",
    "print('  visits:', visits.duplicated().sum())\n",
    "print('  orders:', orders.duplicated().sum())\n",
    "print('  costs :', costs.duplicated().sum())\n",
    "\n",
    "# Clean Visits: remove negative session durations (if any)\n",
    "visits = clean_visits(visits)\n",
    "\n",
    "# Validate date ranges and negative numeric values\n",
    "validate_ranges(visits, orders, costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Column names converted to `snake_case`  \n",
    "- Datetime and numeric types parsed correctly  \n",
    "- No duplicates or missing values found  \n",
    "- Negative session durations removed (2 rows dropped)  \n",
    "- Date ranges across visits, orders, and costs align with the analysis window (Jun 2017 â€“ May 2018)  \n",
    "\n",
    "The data is clean and ready for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Exploring Product Metrics (Usage)\n",
    "\n",
    "In this section, we will answer the following key questions: \n",
    "\n",
    "\n",
    "- How many people use the product every day, week, and month (DAU / WAU / MAU)?\n",
    "- How many sessions occur per day?\n",
    "- What is the average session length?\n",
    "- What is the user retention rate?\n",
    "- How does session length differ by device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. How many people use the product every day, week, and month (DAU / WAU / MAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Activity metrics ---\n",
    "\n",
    "def get_activity(visits: pd.DataFrame):\n",
    "    \"\"\"Return DAU, WAU, MAU as pandas Series.\"\"\"\n",
    "    v = visits.copy()\n",
    "    v['date']  = v['start_ts'].dt.date\n",
    "    v['week']  = v['start_ts'].dt.to_period('W')\n",
    "    v['month'] = v['start_ts'].dt.to_period('M')\n",
    "    \n",
    "    dau = v.groupby('date')['uid'].nunique()\n",
    "    wau = v.groupby('week')['uid'].nunique()\n",
    "    mau = v.groupby('month')['uid'].nunique()\n",
    "    \n",
    "    return dau, wau, mau\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "dau, wau, mau = get_activity(visits)\n",
    "\n",
    "print(f\"DAU â€” Average: {dau.mean():.0f}, Median: {dau.median():.0f}\")\n",
    "print(f\"WAU â€” Average: {wau.mean():.0f}, Median: {wau.median():.0f}\")\n",
    "print(f\"MAU â€” Average: {mau.mean():.0f}, Median: {mau.median():.0f}\")\n",
    "\n",
    "\n",
    "# --- Visualization (DAU/WAU/MAU) ---\n",
    "\n",
    "# 1. Prepare x-axes\n",
    "x_dau = pd.to_datetime(dau.index)\n",
    "x_wau = wau.index.to_timestamp()\n",
    "x_mau = mau.index.to_timestamp()\n",
    "\n",
    "# 2. Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.07,\n",
    "    subplot_titles=[\n",
    "        \"DAU â€” Daily Active Users\",\n",
    "        \"WAU â€” Weekly Active Users\",\n",
    "        \"MAU â€” Monthly Active Users\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Add traces\n",
    "fig.add_trace(go.Scatter(x=x_dau, y=dau, mode=\"lines\", line=dict(color=\"royalblue\")), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=x_wau, y=wau, marker_color=\"royalblue\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_mau, y=mau, mode=\"lines+markers\", line=dict(color=\"royalblue\")), row=3, col=1)\n",
    "\n",
    "# 4. Layout â€” keep only unique settings\n",
    "fig.update_layout(\n",
    "    title=\"DAU, WAU, MAU Metrics\",\n",
    "    height=750,       # custom height for this figure only\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# 5. Axis labels\n",
    "fig.update_yaxes(title_text=\"Users\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Users\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Users\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- The DAU chart turned out too dense to be informative, so we rely on aggregated metrics instead.  \n",
    "Both mean and median values are close to each other, which indicates no strong outliers.  \n",
    "- WAU grew steadily from summer to autumn 2017, peaking in November before showing fluctuations and a gradual decline in spring 2018. The November spike should be further investigated to understand the drivers of user growth.  \n",
    "- MAU followed a similar pattern: steady growth in 2017 with a peak in Novemberâ€“December, followed by a decline in 2018. This correlates with the weekly chart and may reflect seasonality. It is worth analyzing the reasons behind the decline after the peak period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Number of sessions per day\n",
    "\n",
    "We measure daily load as the total number of sessions per calendar day.  \n",
    "To reduce day-to-day noise, we also show a 7-day rolling average and the median as a reference line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Sessions Analysis\n",
    "\n",
    "# Prepare the date column\n",
    "visits['date'] = visits['start_ts'].dt.date\n",
    "\n",
    "# Count the number of sessions per day\n",
    "sessions_per_day = (\n",
    "    visits['date']\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Compute 7-day rolling mean (smoothing)\n",
    "spd_smoothed = sessions_per_day.rolling(7).mean()\n",
    "\n",
    "# Median daily sessions (for reference line)\n",
    "median_spd = sessions_per_day.median()\n",
    "\n",
    "# Print numeric summary\n",
    "print(f\"Median number of sessions per day: {median_spd:,.0f}\")\n",
    "\n",
    "# Plot\n",
    "fig = px.line(\n",
    "    spd_smoothed,\n",
    "    labels={'index': 'Date', 'value': 'Sessions'},\n",
    "    title='Number of Sessions per Day (7-Day Rolling Average)'\n",
    ")\n",
    "\n",
    "# Style line\n",
    "fig.update_traces(line=dict(color='royalblue'))\n",
    "\n",
    "# Add horizontal median line\n",
    "fig.add_hline(\n",
    "    y=median_spd,\n",
    "    line_dash='dash',\n",
    "    line_color='red',\n",
    "    annotation_text='Median',\n",
    "    annotation_position='top left'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Daily sessions show clear fluctuations with prominent peaks around Nov 2017 and early 2018.  \n",
    "- The median daily level is ~1,003 sessions (red dashed line), which serves as a stable baseline.  \n",
    "- The 7-day smoothing helps reveal the overall trend without day-to-day noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Exploring Session Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate session duration (in minutes)\n",
    "session_durations = (visits['end_ts'] - visits['start_ts']) / pd.Timedelta(minutes=1)\n",
    "session_durations = session_durations.astype(int)\n",
    "\n",
    "# Compute summary statistics\n",
    "mean_duration = session_durations.mean()\n",
    "median_duration = session_durations.median()\n",
    "min_duration = session_durations.min()\n",
    "max_duration = session_durations.max()\n",
    "\n",
    "# Print stats\n",
    "print(\"Session Duration Summary (minutes):\")\n",
    "print(f\"Average: {mean_duration:.0f}\")\n",
    "print(f\"Median:  {median_duration:.0f}\")\n",
    "print(f\"Minimum: {min_duration:.0f}\")\n",
    "print(f\"Maximum: {max_duration:.0f}\")\n",
    "\n",
    "# Plot histogram\n",
    "fig = px.histogram(\n",
    "    session_durations,\n",
    "    nbins=200,\n",
    "    range_x=[-1, 120], \n",
    "    title='Distribution of Session Durations',\n",
    "    labels={'value': 'Session duration (minutes)', 'count': 'Number of sessions'}\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_traces(marker_color='royalblue', opacity=0.8)\n",
    "\n",
    "# median line\n",
    "fig.add_vline(\n",
    "    x=median_duration,\n",
    "    line_color='green',\n",
    "    line_dash='dash',\n",
    "    annotation_text='Median'\n",
    ")\n",
    "\n",
    "# mean line\n",
    "fig.add_vline(\n",
    "    x=mean_duration,\n",
    "    line_color='red',\n",
    "    line_dash='dash',\n",
    "    annotation_text='Mean'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "The average session length is 11 minutes, while the median is only 5 minutes. This indicates that most sessions are short, but there are a few very long sessions (up to 711 minutes) that pull the average upward. The minimum duration is 0 minutes, which may correspond to incomplete or instantly closed sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. User Retention Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helper function: compute retention pivot and average retention\n",
    "def compute_retention(visits: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute weekly retention for user cohorts.\n",
    "    \n",
    "    Returns:\n",
    "      retention_pivot (DataFrame): retention by cohort_week Ã— lifetime_week\n",
    "      avg_retention   (Series): average retention per lifetime_week\n",
    "    \"\"\"\n",
    "    v = visits.copy()\n",
    "    \n",
    "    # First activity date for each user\n",
    "    v['first_activity_date'] = v.groupby('uid')['start_ts'].transform('min')\n",
    "    \n",
    "    # Cohort week (first activity week) and activity week\n",
    "    v['first_activity_week'] = v['first_activity_date'].dt.to_period('W').dt.start_time\n",
    "    v['activity_week'] = v['start_ts'].dt.to_period('W').dt.start_time\n",
    "    \n",
    "    # Lifetime in weeks (how many weeks since the first activity)\n",
    "    v['cohort_lifetime'] = (\n",
    "        (v['activity_week'] - v['first_activity_week']) / np.timedelta64(1, 'W')\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Count active users per cohort_week Ã— lifetime_week\n",
    "    cohorts = (\n",
    "        v.groupby(['first_activity_week', 'cohort_lifetime'])['uid']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Cohort size = number of users in week 0 for each cohort\n",
    "    cohorts['cohort_users'] = cohorts.groupby('first_activity_week')['uid'].transform('first')\n",
    "    \n",
    "    # Retention rate = active users / cohort size\n",
    "    cohorts['retention'] = cohorts['uid'] / cohorts['cohort_users']\n",
    "    \n",
    "    # Pivot: rows = cohort start week, columns = lifetime weeks\n",
    "    retention_pivot = cohorts.pivot(\n",
    "        index='first_activity_week',\n",
    "        columns='cohort_lifetime',\n",
    "        values='retention'\n",
    "    ).round(3)\n",
    "    \n",
    "    # Average retention across cohorts for each lifetime week\n",
    "    avg_retention = retention_pivot.mean(axis=0)\n",
    "    \n",
    "    return retention_pivot, avg_retention\n",
    "\n",
    "\n",
    "# Compute retention tables\n",
    "retention_pivot, avg_retention = compute_retention(visits)\n",
    "\n",
    "# Show a sample: first 8 cohorts Ã— first 12 weeks\n",
    "display(retention_pivot.iloc[:8, :12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention heatmap (first 12 cohorts Ã— first 12 weeks)\n",
    "\n",
    "# Make a copy and format index\n",
    "heatmap_data = retention_pivot.copy()\n",
    "heatmap_data.index = pd.to_datetime(heatmap_data.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# Limit to first 12 cohorts and first 12 lifetime weeks\n",
    "heatmap_sample = heatmap_data.iloc[:12, :12]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(13, 9))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_sample,\n",
    "    annot=True,  \n",
    "    fmt='.0%',\n",
    "    cmap='Blues',\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={'label': 'Retention rate'}\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_title(\n",
    "    'User Retention by Cohort and Lifetime Week\\n(first 12 cohorts Ã— first 12 weeks)',\n",
    "    fontsize=16,\n",
    "    pad=30\n",
    ")\n",
    "ax.set_xlabel('Cohort lifetime (weeks)', fontsize=12)\n",
    "ax.set_ylabel('Cohort (first activity week)', fontsize=12)\n",
    "\n",
    "# Show x-axis labels on top\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "# Keep tick labels horizontal\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ“Œ Note:** The table above shows only a sample (first 8 cohorts, 12 weeks). The full retention table is provided in the **Appendix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average retention curve (first 12 weeks)\n",
    "\n",
    "# Take first 12 lifetime weeks\n",
    "avg_ret_12 = avg_retention.iloc[:12]\n",
    "\n",
    "# Prepare a small DataFrame for plotting\n",
    "avg_ret_df = (\n",
    "    avg_ret_12\n",
    "    .reset_index()\n",
    "    .rename(columns={'cohort_lifetime': 'lifetime_week', 0: 'retention'})\n",
    ")\n",
    "\n",
    "# Line plot of average retention\n",
    "fig = px.line(\n",
    "    avg_ret_df,\n",
    "    x='lifetime_week',\n",
    "    y='retention',\n",
    "    markers=True,\n",
    "    title='Average User Retention (first 12 weeks)',\n",
    "    labels={\n",
    "        'lifetime_week': 'Cohort lifetime (weeks)',\n",
    "        'retention': 'Retention rate'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Style the line (color)\n",
    "fig.update_traces(line=dict(color='royalblue'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "**Average Retention Curve**\n",
    "\n",
    "- The chart shows that retention drops steeply after the first week: from 100% in week 0 to under 10% in week 1.\n",
    "- After that, retention stabilizes at a very low level (1â€“2%) and remains nearly flat over the following weeks.\n",
    "\n",
    "**Cohort Heatmap**\n",
    "\n",
    "- The heatmap illustrates the retention dynamics for different cohorts.\n",
    "- The decline pattern is consistent across all cohorts: high churn after the first week, followed by stabilization at a low level.\n",
    "\n",
    "No cohort demonstrates a significantly different behavior, indicating that the retention issue is structural rather than specific to a certain cohort.\n",
    "\n",
    "**Conclusion:**\n",
    "Overall, user retention is extremely low. Almost 90â€“95% of users drop off after the first week, and only about 1â€“2% remain active in the long term. This highlights the need for stronger onboarding and engagement strategies to increase user stickiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Session Length by Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate session duration stats by device\n",
    "stats_by_device = (\n",
    "    visits.groupby('device', observed=True)['session_duration_min']\n",
    "          .agg(['count', 'mean', 'median'])\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "display(stats_by_device.round(1))\n",
    "\n",
    "# Boxplot\n",
    "fig = px.box(\n",
    "    visits,\n",
    "    x='device',\n",
    "    y='session_duration_min',\n",
    "    title='Session Length Distribution by Device',\n",
    "    labels={\n",
    "        'device': 'Device',\n",
    "        'session_duration_min': 'Session length (minutes)'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Limit y-axis to remove extreme outliers\n",
    "fig.update_yaxes(range=[0, 60])\n",
    "\n",
    "# Style the box color\n",
    "fig.update_traces(marker_color='royalblue')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**  \n",
    "\n",
    "Desktop sessions are longer: the median is about 6 minutes (IQR up to ~12â€“15), while touch sessions have a median of only 3 minutes.  \n",
    "\n",
    "This means desktop users spend roughly twice as much time per visit, suggesting deeper engagement on desktop compared to mobile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Exploring Sales Metrics\n",
    "\n",
    "In this section, we will answer the following key questions:\n",
    "\n",
    "- When do people start buying? (conversion delay between registration and first purchase)  \n",
    "- How many orders do users place over time?  \n",
    "- What is the average purchase size (AOV)?  \n",
    "- How much money do users bring overall (LTV)?  \n",
    "- How does the average purchase size differ by device?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Conversion Delay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build user-level table with first activity and first order dates\n",
    "first_activity = (\n",
    "    visits\n",
    "    .groupby('uid')['start_ts']\n",
    "    .min()\n",
    "    .rename('first_activity_date')\n",
    ")\n",
    "\n",
    "first_order = (\n",
    "    orders\n",
    "    .groupby('uid')['buy_ts']\n",
    "    .min()\n",
    "    .rename('first_order_datetime')\n",
    ")\n",
    "\n",
    "users_conv = (\n",
    "    first_activity\n",
    "    .to_frame() \n",
    "    .merge(first_order, on='uid', how='left')\n",
    ")\n",
    "\n",
    "# Calculate conversion delay in days\n",
    "users_conv['conversion_delay_days'] = (\n",
    "    users_conv['first_order_datetime'] - users_conv['first_activity_date']\n",
    ").dt.days\n",
    "\n",
    "# Define buckets for conversion delay\n",
    "bins = [-np.inf, -0.5, 0.5, 1.5, 3.5, 7.5, 30.5, 90.5, np.inf]\n",
    "labels = ['No purchase', '0d', '1d', '2-3d', '4-7d', '8-30d', '31-90d', '90d+']\n",
    "\n",
    "# Put each user into a conversion bucket\n",
    "# If user has no purchase (NaN), we replace by -1 to fall into \"No purchase\"\n",
    "users_conv['conversion_bucket'] = pd.cut(\n",
    "    users_conv['conversion_delay_days'].fillna(-1),\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# Calculate distribution by buckets (counts and shares)\n",
    "conv_counts = users_conv['conversion_bucket'].value_counts(sort=False)\n",
    "conv_shares = (\n",
    "    users_conv['conversion_bucket']\n",
    "    .value_counts(sort=False, normalize=True) * 100\n",
    ")\n",
    "\n",
    "conv_df = (\n",
    "    pd.DataFrame({\n",
    "        'conversion_bucket': conv_counts.index,\n",
    "        'count': conv_counts.values,\n",
    "        'share': conv_shares.values\n",
    "    })\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Look at table: how many users in each bucket\n",
    "display(conv_df[['conversion_bucket', 'count']])\n",
    "\n",
    "# Plot conversion delay distribution\n",
    "fig = px.bar(\n",
    "    conv_df,\n",
    "    x='conversion_bucket',\n",
    "    y='share',\n",
    "    title='Conversion Delay Distribution (%)',\n",
    "    labels={\n",
    "        'conversion_bucket': 'Conversion delay',\n",
    "        'share': 'Share of users (%)'\n",
    "    },\n",
    "    text='share'\n",
    ")\n",
    "\n",
    "# Style bars and labels\n",
    "fig.update_traces(\n",
    "    texttemplate='%{text:.1f}%',\n",
    "    marker_color='royalblue',\n",
    "    hovertemplate='%{x}: %{y:.1f}%<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Most users never make a purchase after registration â€” 84% fall into the \"No purchase\" category.\n",
    "Among those who convert, the majority do so on the same day of registration (â‰ˆ12%).\n",
    "Only a very small share of users postpone their first purchase:\n",
    "\n",
    "- 0.4â€“0.5% within 1â€“7 days\n",
    "- 1% within 8â€“90 days\n",
    "- 1.2% after 90+ days.\n",
    "\n",
    "This indicates that conversion happens almost immediately if it happens at all. Long-term conversions are extremely rare, and the main focus should be on motivating users to purchase on day 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Number of Orders per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many orders each user made\n",
    "orders_per_user = (\n",
    "    orders\n",
    "    .groupby('uid')\n",
    "    .size()\n",
    "    .reset_index(name='orders_count')\n",
    ")\n",
    "\n",
    "# Calculate share of users by number of orders\n",
    "order_dist = (\n",
    "    orders_per_user['orders_count']\n",
    "    .value_counts(normalize=True)\n",
    "    .reset_index(name='share')\n",
    "    .rename(columns={'index': 'orders_count'})\n",
    "    .sort_values('orders_count')\n",
    ")\n",
    "\n",
    "# Keep only users with up to 10 orders (for readability)\n",
    "max_orders = 10\n",
    "dist_df = (\n",
    "    order_dist[\n",
    "        (order_dist['orders_count'] <= max_orders) & \n",
    "        (order_dist['share'] > 0)\n",
    "    ]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Plot distribution (bar chart)\n",
    "fig = px.bar(\n",
    "    dist_df,\n",
    "    x=\"orders_count\",\n",
    "    y=\"share\",\n",
    "    title=\"Distribution of Orders per User (up to 10 orders)\",\n",
    "    labels={\n",
    "        \"orders_count\": \"Number of orders\",\n",
    "        \"share\": \"Share of users\"\n",
    "    },\n",
    "    text=\"share\"\n",
    ")\n",
    "\n",
    "# Style bars and labels\n",
    "fig.update_xaxes(tickmode=\"linear\", dtick=1)\n",
    "fig.update_traces(\n",
    "    marker_color=\"royalblue\",\n",
    "    texttemplate='%{text:.1%}',\n",
    "    textposition=\"outside\"\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickformat=\".0%\",\n",
    "    range=[0, dist_df['share'].max() * 1.2]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- The majority of users (â‰ˆ83%) made only one order.\n",
    "- About 12% of users made two orders, and the share drops sharply after that.\n",
    "- Fewer than 1% of users placed more than five orders.\n",
    "- There were isolated cases of users making over 100 orders, but their share is statistically insignificant (<0.1%) and was excluded from the chart for clarity.\n",
    "\n",
    "Overall, the distribution of orders per user is highly skewed towards a single order, highlighting a low level of repeat purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Average Purchase Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall AOV for the whole period\n",
    "# Formula: total revenue / total number of orders\n",
    "total_revenue = orders['revenue'].sum()\n",
    "total_orders = orders['revenue'].count()\n",
    "aov = total_revenue / total_orders\n",
    "\n",
    "print(f\"Average Order Value (AOV): {aov:.2f}\")\n",
    "\n",
    "# AOV by month\n",
    "# Ð¡reate a copy and add month column\n",
    "orders_monthly = orders.copy()\n",
    "orders_monthly['month'] = orders_monthly['buy_ts'].dt.to_period('M')\n",
    "\n",
    "# Ð¡alculate AOV per month:\n",
    "# sum(revenue) / number of orders in each month\n",
    "aov_by_month = (\n",
    "    orders_monthly\n",
    "    .groupby('month')['revenue']\n",
    "    .agg(['sum', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "aov_by_month['aov'] = (aov_by_month['sum'] / aov_by_month['count']).round(2)\n",
    "\n",
    "# Ð¡onvert Period ('month') â†’ Timestamp (first day of the month)\n",
    "aov_by_month['month'] = aov_by_month['month'].dt.to_timestamp()\n",
    "\n",
    "# Ðžptional â€” drop incomplete last month\n",
    "last_date = orders_monthly['buy_ts'].max()\n",
    "end_of_last_month = last_date.to_period('M').to_timestamp(how='end')\n",
    "\n",
    "# If the last month is incomplete, remove it from the chart\n",
    "if last_date < end_of_last_month:\n",
    "    last_full_month_start = (last_date.to_period('M') - 1).to_timestamp()\n",
    "    aov_by_month = aov_by_month[aov_by_month['month'] <= last_full_month_start]\n",
    "\n",
    "# Keep only the columns we need for plotting\n",
    "aov_plot_df = aov_by_month[['month', 'aov']]\n",
    "\n",
    "# Plot AOV by month\n",
    "fig = px.line(\n",
    "    aov_plot_df,\n",
    "    x='month',\n",
    "    y='aov',\n",
    "    markers=True,\n",
    "    title='Average Order Value (AOV) by Month',\n",
    "    labels={\n",
    "        'month': 'Month',\n",
    "        'aov': 'Average Order Value'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Style line + hover\n",
    "fig.update_traces(\n",
    "    line=dict(width=3, color='royalblue'),\n",
    "    marker=dict(size=6),\n",
    "    hovertemplate='Month: %{x|%Y-%m}<br>AOV: %{y:.2f}<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickformat=\"%Y-%m\")\n",
    "fig.update_yaxes(tickformat=\".2f\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- The analysis shows that the average order value (AOV) is about **5**.  \n",
    "- When looking at the monthly trend, the AOV fluctuates between 4 and 6 units without a clear upward or downward pattern.  \n",
    "- This suggests that the typical purchase size remained relatively stable during the observed period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Customer Lifetime Value (LTV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month of each order\n",
    "orders['month'] = orders['buy_ts'].dt.to_period('M')\n",
    "\n",
    "# Define cohort:\n",
    "# First_order_month = first purchase month for each user\n",
    "orders['first_order_month'] = (\n",
    "    orders\n",
    "    .groupby('uid')['month']\n",
    "    .transform('min')\n",
    ")\n",
    "\n",
    "# Cohort lifetime in months:\n",
    "# We convert Period to integer codes and subtract\n",
    "orders['cohort_lifetime'] = (\n",
    "    orders['month'].astype('int64') \n",
    "    - orders['first_order_month'].astype('int64')\n",
    ")\n",
    "\n",
    "# Cohort size = number of unique users in each first_order_month\n",
    "cohort_users = (\n",
    "    orders\n",
    "    .groupby('first_order_month')['uid']\n",
    "    .nunique()\n",
    ")\n",
    "\n",
    "# Total revenue per cohort and lifetime month\n",
    "cohort_revenue = (\n",
    "    orders\n",
    "    .groupby(['first_order_month', 'cohort_lifetime'])['revenue']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add cohort size to each row\n",
    "cohort_revenue['cohort_users'] = cohort_revenue['first_order_month'].map(cohort_users)\n",
    "\n",
    "# LTV for each cohort & lifetime month:\n",
    "# LTV_month = cohort_revenue / cohort_users\n",
    "cohort_revenue['ltv'] = (\n",
    "    cohort_revenue['revenue'] / cohort_revenue['cohort_users']\n",
    ").round(2)\n",
    "\n",
    "# Pivot table:\n",
    "# Rows = cohorts (first_order_month), columns = lifetime months, values = LTV\n",
    "ltv_pivot = cohort_revenue.pivot_table(\n",
    "    index='first_order_month',\n",
    "    columns='cohort_lifetime',\n",
    "    values='ltv'\n",
    ")\n",
    "\n",
    "# Cumulative LTV across lifetime months\n",
    "ltv_cum = ltv_pivot.cumsum(axis=1).round(2)\n",
    "\n",
    "# Average cumulative LTV at month 6 across cohorts (sanity check)\n",
    "if 6 in ltv_cum.columns:\n",
    "    avg_ltv_6m = ltv_cum[6].dropna().mean()\n",
    "    print(f\"Average cumulative LTV at month 6 across cohorts: {avg_ltv_6m:.2f}\\n\")\n",
    "else:\n",
    "    print(\"Month 6 is not available in the dataset.\\n\")\n",
    "\n",
    "# Show the cumulative LTV table\n",
    "display(ltv_cum)\n",
    "\n",
    "# Plot cumulative LTV heatmap\n",
    "\n",
    "# Make a copy and format index\n",
    "ltv_for_plot = ltv_cum.copy()\n",
    "ltv_for_plot.index = (\n",
    "    pd.to_datetime(ltv_for_plot.index.astype(str))\n",
    "    .strftime('%Y-%m')\n",
    ")\n",
    "\n",
    "# Draw heatmap\n",
    "plt.figure(figsize=(13, 9))\n",
    "ax = sns.heatmap(\n",
    "    ltv_for_plot,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={'label': 'Cumulative LTV'}\n",
    ")\n",
    "\n",
    "# Titles and axis labels\n",
    "ax.set_title(\n",
    "    'Cumulative LTV by Cohort and Lifetime Month',\n",
    "    fontsize=16,\n",
    "    pad=30\n",
    ")\n",
    "ax.set_xlabel('Cohort lifetime (months)', fontsize=12)\n",
    "ax.set_ylabel('Cohort (first order month)', fontsize=12)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- Cumulative LTV analysis shows that customers bring most of their value within the first 3â€“4 months of their lifetime.\n",
    "- On average, a customer contributes 5â€“8 units in total, with the most valuable cohort (September 2017) reaching nearly 13 units. This means that the majority of revenue is generated shortly after acquisition, while long-term contribution is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Average Order Value (AOV) by Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring device info into the orders table (join by user ID)\n",
    "# We use drop_duplicates() just in case the same user has many sessions.\n",
    "orders_with_device = orders.merge(\n",
    "    visits[['uid', 'device']].drop_duplicates(),\n",
    "    on='uid',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate AOV by device:\n",
    "# AOV_device = average order revenue per device\n",
    "aov_by_device = (\n",
    "    orders_with_device\n",
    "    .groupby('device', observed=False)['revenue']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(aov_by_device)\n",
    "\n",
    "# Prepare max value for x-axis\n",
    "max_x = float(aov_by_device['revenue'].max()) * 1.2\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "fig = px.bar(\n",
    "    aov_by_device,\n",
    "    x='revenue',\n",
    "    y='device',\n",
    "    orientation='h',\n",
    "    title='Average Order Value (AOV) by Device',\n",
    "    labels={'device': 'Device', 'revenue': 'AOV'},\n",
    "    text='revenue'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textposition='outside',\n",
    "    marker_color='royalblue'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0, max_x])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- Average purchase size is higher on touch devices (â‰ˆ10.6) compared to desktop (â‰ˆ9.0).  \n",
    "- This suggests that mobile users, although often less engaged per session, tend to make slightly larger purchases when they do order.  \n",
    "- Desktop users generate more sessions overall, but their individual orders are smaller on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4. Marketing Metrics Analysis\n",
    "\n",
    "- How much was spent? (overall, by source, and over time)  \n",
    "- What was the customer acquisition cost (CAC)? (per source)  \n",
    "- What was the return on investment (ROI) and payback period by cohorts?  \n",
    "- How is marketing spend distributed by device?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Analysis of Marketing Expenditures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall spend for the whole period\n",
    "total_cost = int(costs['costs'].sum())\n",
    "print(f\"Overall marketing costs: {total_cost:,.0f}\")\n",
    "\n",
    "# Spend by source\n",
    "# Group costs by source_id and sort descending\n",
    "costs_by_sources = (\n",
    "    costs.groupby('source_id', observed=False)['costs']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values('costs', ascending=False)\n",
    ")\n",
    "\n",
    "# Convert source_id to string so it looks nicer on the chart\n",
    "costs_by_sources['source_id'] = costs_by_sources['source_id'].astype(str)\n",
    "\n",
    "# Plot total spend by source\n",
    "fig = px.bar(\n",
    "    costs_by_sources,\n",
    "    x='source_id',\n",
    "    y='costs',\n",
    "    title='Marketing Costs by Source',\n",
    "    labels={'source_id': 'Source', 'costs': 'Costs'},\n",
    "    text='costs'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker_color='royalblue',\n",
    "    texttemplate='%{text:,.0f}',\n",
    "    hovertemplate='Source %{x}<br>Costs: %{y:,.0f}<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(tickformat=',.0f')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Spend by month (total)\n",
    "# Create a month period column from date\n",
    "costs['month'] = costs['dt'].dt.to_period('M')\n",
    "\n",
    "# Group by month and sum costs\n",
    "costs_by_month = (\n",
    "    costs.groupby('month', observed=False)['costs']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert Period to Timestamp \n",
    "costs_by_month['month'] = costs_by_month['month'].dt.to_timestamp()\n",
    "\n",
    "# Plot total marketing costs over time\n",
    "fig = px.bar(\n",
    "    costs_by_month,\n",
    "    x='month',\n",
    "    y='costs',\n",
    "    title='Marketing Costs Over Time',\n",
    "    labels={'month': 'Month', 'costs': 'Costs'},\n",
    "    text='costs'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker_color='royalblue',\n",
    "    texttemplate='%{text:,.0f}',\n",
    "    hovertemplate='Month %{x|%Y-%m}<br>Costs: %{y:,.0f}<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(tickformat=',.0f')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Spend by month and source\n",
    "# Group by month and source_id\n",
    "costs_msrc = (\n",
    "    costs.groupby(['month', 'source_id'], observed=False)['costs']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert month to timestamp and source_id to string\n",
    "costs_msrc['month'] = costs_msrc['month'].dt.to_timestamp()\n",
    "costs_msrc['source_id'] = costs_msrc['source_id'].astype(str)\n",
    "\n",
    "# Plot stacked area chart: costs over time by source\n",
    "fig = px.area(\n",
    "    costs_msrc.sort_values('month'),\n",
    "    x='month',\n",
    "    y='costs',\n",
    "    color='source_id',\n",
    "    title='Monthly Marketing Costs by Source',\n",
    "    labels={'month': 'Month', 'costs': 'Costs', 'source_id': 'Source'}\n",
    ")\n",
    "\n",
    "fig.update_yaxes(tickformat=',.0f')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- The total marketing spend during the observed period amounted to 329,131.\n",
    "- Most of the budget was allocated to a few major sources: Source 3 dominated with â‰ˆ141k (â‰ˆ43% of total), followed by Source 4 (â‰ˆ61k) and Source 5 (â‰ˆ52k). Other sources accounted for much smaller shares, each contributing less than 15% of the total budget.\n",
    "- Spending dynamics show a clear peak in Novemberâ€“December 2017 (â‰ˆ37â€“38k per month) followed by a gradual decline in early 2018, indicating that the most intensive investment period was concentrated in late 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Customer Acquisition Cost (CAC) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First order date per user\n",
    "first_order = (\n",
    "    orders.groupby('uid', as_index=False)['buy_ts']\n",
    "    .min()\n",
    "    .rename(columns={'buy_ts': 'first_order_datetime'})\n",
    ")\n",
    "\n",
    "# Source of first purchase per user\n",
    "users_sources = (\n",
    "    first_order\n",
    "    .merge(\n",
    "        visits[['uid', 'start_ts', 'source_id']],\n",
    "        on='uid',\n",
    "        how='left'\n",
    "    )\n",
    "    .query('start_ts <= first_order_datetime')\n",
    "    .sort_values(['uid', 'start_ts'])\n",
    "    .drop_duplicates('uid')[['uid', 'source_id']]\n",
    ")\n",
    "\n",
    "# New customers per source (ÐºÐ¾Ð»-Ð²Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾ÐºÑƒÐ¿Ð°Ñ‚ÐµÐ»ÐµÐ¹)\n",
    "total_new_users_by_src = (\n",
    "    users_sources.groupby('source_id', as_index=False)['uid']\n",
    "    .nunique()\n",
    "    .rename(columns={'uid': 'new_customers'})\n",
    ")\n",
    "\n",
    "# Total marketing spend per source\n",
    "total_costs_by_src = (\n",
    "    costs.groupby('source_id', as_index=False)['costs']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# CAC = total spend / number of new customers\n",
    "cac_overall = total_costs_by_src.merge(\n",
    "    total_new_users_by_src,\n",
    "    on='source_id',\n",
    "    how='left'\n",
    ")\n",
    "cac_overall['CAC'] = (cac_overall['costs'] / cac_overall['new_customers']).round(2)\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_df = cac_overall[['source_id', 'new_customers', 'CAC']].copy()\n",
    "plot_df['source_id'] = plot_df['source_id'].astype(str)\n",
    "\n",
    "# Plot\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Bar chart â€” New customers per source\n",
    "fig.add_bar(\n",
    "    x=plot_df['source_id'],\n",
    "    y=plot_df['new_customers'],\n",
    "    name='New Customers',\n",
    "    marker_color='royalblue',\n",
    "    text=plot_df['new_customers'],\n",
    "    textposition='outside',\n",
    "    hovertemplate='Source=%{x}<br>New customers=%{y:,}<extra></extra>'\n",
    ")\n",
    "\n",
    "# Line chart â€” CAC per source\n",
    "fig.add_scatter(\n",
    "    x=plot_df['source_id'],\n",
    "    y=plot_df['CAC'],\n",
    "    name='CAC',\n",
    "    mode='lines+markers',\n",
    "    line=dict(width=2, color='red'),\n",
    "    marker=dict(size=7, color='red'),\n",
    "    hovertemplate='Source=%{x}<br>CAC=%{y:.2f}<extra></extra>',\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    title='New Customers and CAC by Source',\n",
    "    height=450,\n",
    "    margin=dict(l=60, r=20, t=60, b=60),\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- **Source 4** stands out as the most efficient channel, bringing in over 10k new customers with one of the lowest CAC values (~5.9).\n",
    "- **Sources 9 and 10** also show very low CAC (~4â€“5), but their scale is limited.\n",
    "- In contrast, **Source 3** attracted a similar number of customers as Source 4, yet with the highest CAC (~13.5), making it the least cost-effective option.\n",
    "- **Sources 1, 2, and 5** delivered moderate results, with a reasonable balance of spend and customers but room for improvement.\n",
    "\n",
    "**Recommendations:** \n",
    "- Focus on scaling Source 4, as it combines strong reach with low acquisition costs.\n",
    "- Maintain Sources 9 and 10 due to their efficiency and explore whether they can be scaled.\n",
    "- Reassess investments in Source 3 to optimize or reduce spending.\n",
    "- Continue using Sources 1, 2, and 5, but test adjustments (targeting, segmentation, creatives) to improve CAC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Marketing ROI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare costs per cohort (use monthly costs as \"cohort costs\")\n",
    "cohort_costs = costs_by_month.rename(columns={'month': 'first_order_month'})\n",
    "cohort_costs['first_order_month'] = cohort_costs['first_order_month'].dt.to_period('M')\n",
    "\n",
    "# Merge cohort revenue (ltv by cohort_lifetime) with monthly costs\n",
    "report_roi = cohort_revenue.merge(\n",
    "    cohort_costs,\n",
    "    on='first_order_month',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Number of users in each cohort (first_order_month)\n",
    "cohort_users = (\n",
    "    orders\n",
    "    .groupby('first_order_month')['uid']\n",
    "    .nunique()\n",
    ")\n",
    "\n",
    "# Add users column to report (cohort size)\n",
    "report_roi['users'] = report_roi['first_order_month'].map(cohort_users)\n",
    "\n",
    "# CAC per cohort (how much we spent per 1 user in cohort)\n",
    "report_roi['cac'] = (report_roi['costs'] / report_roi['users']).round(2)\n",
    "\n",
    "# Instant ROI per cohortâ€“month\n",
    "# ROI = LTV(month) / CAC\n",
    "report_roi['roi'] = (report_roi['ltv'] / report_roi['cac']).round(2)\n",
    "\n",
    "# Cumulative LTV and cumulative ROI over lifetime months\n",
    "report_roi = report_roi.sort_values(['first_order_month', 'cohort_lifetime'])\n",
    "\n",
    "report_roi['ltv_cum'] = (\n",
    "    report_roi\n",
    "    .groupby('first_order_month')['ltv']\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "report_roi['roi_cum'] = (report_roi['ltv_cum'] / report_roi['cac']).round(2)\n",
    "\n",
    "# Prepare pivot table for heatmap (rows = cohorts, columns = lifetime months)\n",
    "roi_cum_pivot = report_roi.pivot_table(\n",
    "    index='first_order_month',\n",
    "    columns='cohort_lifetime',\n",
    "    values='roi_cum'\n",
    ").round(2)\n",
    "\n",
    "display(roi_cum_pivot)\n",
    "\n",
    "# Plot heatmap of cumulative ROI by cohort\n",
    "plt.figure(figsize=(13, 9))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    roi_cum_pivot,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={'label': 'Cumulative ROI'}\n",
    ")\n",
    "\n",
    "ax.set_title('Cumulative ROI by Cohort and Lifetime Month', fontsize=16, pad=30)\n",
    "ax.set_xlabel('Cohort lifetime (months)', fontsize=12)\n",
    "ax.set_ylabel('Cohort (first order month)', fontsize=12)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "The ROI analysis shows how marketing investments paid off over time across customer cohorts:\n",
    "\n",
    "- Most cohorts started with an ROI below 1.0 in the first months, meaning they were not yet profitable.\n",
    "- Over time, ROI steadily increased as customer revenue accumulated.\n",
    "- Several early cohorts (e.g., Juneâ€“September 2017) reached or exceeded ROI > 1.0 within 6â€“9 months, showing that investments in those months became profitable in the medium term.\n",
    "- Later cohorts (2018) generally demonstrate slower or weaker ROI growth, many staying below 1.0 within the observed period, which suggests longer payback times or less effective marketing spend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. CAC by device (Customer Acquisition Cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First order date per user\n",
    "first_order = (\n",
    "    orders\n",
    "    .groupby('uid', as_index=False)['buy_ts']\n",
    "    .min()\n",
    "    .rename(columns={'buy_ts': 'first_order_datetime'})\n",
    ")\n",
    "\n",
    "# First touch (first visit before first order): we take source and device\n",
    "first_touch = (\n",
    "    first_order\n",
    "    .merge(\n",
    "        visits[['uid', 'start_ts', 'source_id', 'device']],\n",
    "        on='uid',\n",
    "        how='left'\n",
    "    )\n",
    "    .query('start_ts <= first_order_datetime')\n",
    "    .sort_values(['uid', 'start_ts'])\n",
    "    .drop_duplicates('uid')[['uid', 'source_id', 'device']]\n",
    ")\n",
    "\n",
    "# Number of new customers by (source, device)\n",
    "new_users_src_dev = (\n",
    "    first_touch\n",
    "    .groupby(['source_id', 'device'], observed=True)['uid']\n",
    "    .nunique()\n",
    "    .reset_index(name='new_customers')\n",
    ")\n",
    "\n",
    "# Total marketing costs per source\n",
    "costs_by_src = (\n",
    "    costs\n",
    "    .groupby('source_id', as_index=False)['costs']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Allocate costs between devices proportionally to new customers\n",
    "alloc = new_users_src_dev.merge(costs_by_src, on='source_id', how='left')\n",
    "\n",
    "# total new customers per source (for proportional split)\n",
    "alloc['new_customers_src'] = alloc.groupby('source_id', observed=True)['new_customers'].transform('sum')\n",
    "\n",
    "# allocated costs per (source, device)\n",
    "alloc['allocated_costs'] = (\n",
    "    alloc['costs'] * alloc['new_customers'] / alloc['new_customers_src']\n",
    ").round(2)\n",
    "\n",
    "# CAC per device\n",
    "device_alloc = (\n",
    "    alloc\n",
    "    .groupby('device', observed=True, as_index=False)\n",
    "    .agg(\n",
    "        new_customers=('new_customers', 'sum'),\n",
    "        allocated_costs=('allocated_costs', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "device_alloc['cac_device'] = (\n",
    "    device_alloc['allocated_costs'] / device_alloc['new_customers']\n",
    ").round(2)\n",
    "\n",
    "# LTV per device (average revenue per user on this device)\n",
    "user_rev = (\n",
    "    orders\n",
    "    .groupby('uid', as_index=False)['revenue']\n",
    "    .sum()\n",
    "    .rename(columns={'revenue': 'user_revenue'})\n",
    ")\n",
    "\n",
    "user_dev_rev = user_rev.merge(first_touch[['uid', 'device']], on='uid', how='left')\n",
    "\n",
    "ltv_by_device = (\n",
    "    user_dev_rev\n",
    "    .groupby('device', observed=True)['user_revenue']\n",
    "    .mean()\n",
    "    .reset_index(name='ltv_device')\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# ROI = LTV / CAC per device\n",
    "roi_device = ltv_by_device.merge(device_alloc, on='device', how='left')\n",
    "roi_device['roi_device'] = (\n",
    "    roi_device['ltv_device'] / roi_device['cac_device']\n",
    ").round(2)\n",
    "\n",
    "# Final table for analysis\n",
    "cols = ['device', 'ltv_device', 'cac_device',\n",
    "        'new_customers', 'allocated_costs', 'roi_device']\n",
    "\n",
    "display(roi_device[cols].reset_index(drop=True))\n",
    "\n",
    "# Plot ROI by device (horizontal bar chart)\n",
    "# auto-scale x-axis: 120% of max ROI, but not less than 1.0\n",
    "max_x = max(1.0, float(roi_device['roi_device'].max()) * 1.2)\n",
    "\n",
    "fig = px.bar(\n",
    "    roi_device,\n",
    "    x='roi_device',\n",
    "    y='device',\n",
    "    orientation='h',\n",
    "    title='Average ROI by Device',\n",
    "    labels={'device': 'Device', 'roi_device': 'ROI'},\n",
    "    text='roi_device'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textposition='outside',\n",
    "    marker_color='royalblue'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(range=[0, max_x]),\n",
    "    height=400,\n",
    "    margin=dict(l=80, r=20, t=60, b=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**  \n",
    "\n",
    "- Acquisition costs (CAC) are nearly the same for desktop and touch users (â‰ˆ9).  \n",
    "- However, desktop customers generate higher LTV (7.2 vs. 5.6), which leads to a stronger ROI (0.80 vs. 0.64).  \n",
    "- This means that marketing investments in desktop users pay off faster, while touch campaigns remain less efficient within the observed period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Product usage.**  \n",
    "Users generate most of their value during the first 3â€“4 months of activity. After this period, the marginal contribution drops significantly. This highlights the importance of effective onboarding and early engagement campaigns.  \n",
    "\n",
    "**Sales performance.**  \n",
    "On average, a customer contributes 5â€“8 revenue units in their lifetime, with the best cohort (Sep 2017) reaching nearly 13. Most revenue is therefore concentrated in the short-to-medium term, which defines the expected payback horizon.  \n",
    "\n",
    "**Marketing efficiency.**  \n",
    "- The total spend amounted to ~329k, peaking in Novâ€“Dec 2017.  \n",
    "- **Source 4** was the strongest channel: it attracted >10k new users with one of the lowest CAC (~5.9).  \n",
    "- **Sources 9 and 10** were highly efficient (CAC 4â€“5) but small in scale.  \n",
    "- **Source 3** absorbed the largest share of the budget yet had the highest CAC (~13.5), making it the least efficient.  \n",
    "- **Sources 1, 2, and 5** showed average performance with potential for optimization.  \n",
    "- ROI analysis shows that only early cohorts (summerâ€“fall 2017) achieved ROI > 1 within 6â€“9 months. Later cohorts (2018) performed weaker, with many not reaching payback during the observation window.  \n",
    "\n",
    "**Recommendations.**  \n",
    "1. **Scale Source 4** as the most effective channel (low CAC, large user base).  \n",
    "2. **Maintain and explore growth for Sources 9 and 10**, given their efficiency, but assess scalability.  \n",
    "3. **Reduce or optimize Source 3** to avoid overspending on an inefficient channel.  \n",
    "4. **Keep Sources 1, 2, and 5**, but refine targeting and creatives to improve CAC and ROI.  \n",
    "5. **Focus on retention efforts in the first 3â€“4 months**, where most of the user value is generated.  \n",
    "6. Consider adjusting budget allocation after Dec 2017, as later cohorts showed weaker ROI.  \n",
    "\n",
    "**Final note.**  \n",
    "These recommendations are based on comparative analysis of LTV, CAC, ROI, and payback across cohorts and sources. They provide a clear path for reallocating the marketing budget toward higher-yield channels and strengthening early-stage retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retention table (% of users retained)**\n",
    "\n",
    "**Note:**  \n",
    "This table, presented in the appendix, refers to **Step 2.4. User Retention Rate**.  \n",
    "It shows the percentage of users retained from each cohort by week.  \n",
    "Values indicate what share of the original cohort remained active in subsequent weeks.  \n",
    "For example, a value of 10% in week 1 means that only 10% of users who registered in that cohort returned the following week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_pivot.index = pd.to_datetime(retention_pivot.index)\n",
    "display(retention_pivot.style.format('{:.1%}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
